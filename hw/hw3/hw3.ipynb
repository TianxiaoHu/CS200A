{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel$\\rightarrow$Restart) and then **run all cells** (in the menubar, select Cell$\\rightarrow$Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Tianxiao Hu\"\n",
    "COLLABORATORS = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "02301f12b6e9dacaafed23eeef6a2ec1",
     "grade": false,
     "grade_id": "intro",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Homework 3: Loss Minimization\n",
    "## Modeling, Estimation and Gradient Descent\n",
    "## Due Date: Tuesday 10/9, 11:59 PM\n",
    "## Course Policies\n",
    "\n",
    "Here are some important course policies. These are also located at\n",
    "http://www.ds100.org/fa18/.\n",
    "\n",
    "**Collaboration Policy**\n",
    "\n",
    "Data science is a collaborative activity. While you may talk with others about\n",
    "the homework, we ask that you **write your solutions individually**. If you do\n",
    "discuss the assignments with others please **include their names** at the top\n",
    "of your solution.\n",
    "\n",
    "## This Assignment\n",
    "In this homework, we explore modeling data, estimating optimal parameters and a numerical estimation method, gradient descent. These concepts are some of the fundamentals of data science and machine learning and will serve as the building blocks for future projects, classes, and work.\n",
    "\n",
    "After this homework, you should feel comfortable with the following:\n",
    "\n",
    "- Practice reasoning about a model\n",
    "\n",
    "- Build some intuition for loss functions and how they behave \n",
    "\n",
    "- Work through deriving the gradient of a loss with respect to model parameters\n",
    "\n",
    "- Work through a basic version of gradient descent.\n",
    "\n",
    "This homework is comprised of completing code, deriving analytic solutions, writing LaTex and visualizing loss.\n",
    "\n",
    "## Submission - IMPORTANT, PLEASE READ\n",
    "For this assignment and future assignments (homework and projects) you will also submit your free response and plotting questions to gradescope. To do this, you can download as PDF (`File->Download As->PDF via Latex (.pdf)`). You are responsible for submitting and tagging your answers in gradescope. For each free response and plotting question, please include:\n",
    "\n",
    "1. Relevant code used to generate the plot or inform your insights\n",
    "2. The written free response or plot\n",
    "\n",
    "We are doing this to make it easier on our graders and for you, in the case you need to submit a regrade request. Gradescope (as of now) is still better for manual grading.\n",
    "\n",
    "## Score breakdown\n",
    "\n",
    "Question | Points\n",
    "--- | ---\n",
    "Question 1a | 1\n",
    "Question 1b | 1\n",
    "Question 1c | 1\n",
    "Question 1d | 1\n",
    "Question 1e | 1\n",
    "Question 2a | 2\n",
    "Question 2b | 1\n",
    "Question 2c | 1\n",
    "Question 2d | 1\n",
    "Question 2e | 1\n",
    "Question 2f | 1\n",
    "Question 3a | 1\n",
    "Question 3b | 3\n",
    "Question 3c | 2\n",
    "Question 4a | 3\n",
    "Question 4b | 1\n",
    "Question 4c | 1\n",
    "Question 4d | 1\n",
    "Question 4e | 1\n",
    "Question 5a | 2\n",
    "Question 5b | 4\n",
    "Question 5c | 0\n",
    "Question 5d | 0\n",
    "Question 6a | 3\n",
    "Question 6b | 3\n",
    "Question 6c | 3\n",
    "Question 6d | 3\n",
    "Question 6e | 3\n",
    "Question 6f | 3\n",
    "Question 6g | 3\n",
    "Question 7a | 1\n",
    "Question 7b | 1\n",
    "Question 7c | 1\n",
    "Question 7d | 1\n",
    "Question 7e | 0\n",
    "Total | 56"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bac6a451533b61b25b292b429fa0f6d",
     "grade": false,
     "grade_id": "cell-e8cbb07a2cc27f7d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cad69eb59d08b9488842f5208caa88e8",
     "grade": false,
     "grade_id": "imports",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import re\n",
    "import seaborn as sns\n",
    "\n",
    "# Set some parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 9)\n",
    "plt.rcParams['font.size'] = 16\n",
    "np.set_printoptions(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58cd068d0fb2a6f84be08bdc19309d59",
     "grade": false,
     "grade_id": "utils-code",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# We will use plot_3d helper function to help us visualize gradient\n",
    "from hw3_utils import plot_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e50d87c58433d1f61ccb5dbb0909c77a",
     "grade": false,
     "grade_id": "load",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Load Data\n",
    "Load the data.csv file into a pandas dataframe.  \n",
    "Note that we are reading the data directly from the URL address."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91d603b4ab848d73968fbdd35f17e068",
     "grade": false,
     "grade_id": "load-data",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell to load our sample data\n",
    "data = pd.read_csv(\"https://github.com/DS-100/fa18/raw/gh-pages/assets/datasets/hw3_data.csv\", index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d900aec5285d1792733b1d7403564e8a",
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 1: A Simple Model\n",
    "Let's start by examining our data and creating a simple model that can represent this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dc59c834875881164e9d41928ce80bf3",
     "grade": false,
     "grade_id": "q1a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 1\n",
    "#### Question 1a\n",
    "First, let's visualize the data in a scatter plot. After implementing the `scatter` function below, you should see something like this:\n",
    "![scatter](scatter.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "031194d8ce49fedb13c7b92b1a41d093",
     "grade": true,
     "grade_id": "q1a-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def scatter(x, y):\n",
    "    \"\"\"\n",
    "    Generate a scatter plot using x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ...\n",
    "    plt.scatter(x, y, s=10)\n",
    "    plt.show()\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "x = data['x']\n",
    "y = data['y']\n",
    "scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fa35bf9191846b4990718a81020f4c03",
     "grade": false,
     "grade_id": "q1b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 1b\n",
    "Describe any significant observations about the distribution of the data. How can you describe the relationship between $x$ and $y$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4d45c015db20db03a69c6b92a60bdd8d",
     "grade": true,
     "grade_id": "q1b-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "As $x$ goes up, there is a trend of $y$ for going up. The relationship is like a $sin$ wave plus a line with positive slope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "01c87f1d2001702454ab3600d5eb82ed",
     "grade": false,
     "grade_id": "q1c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 1c\n",
    "The data looks roughly linear, with some extra noise. For now, let's assume that the data follows some underlying linear model. We define the underlying linear model that predicts the value $y$ using the value $x$ as: $f_{\\theta^*}(x) = \\theta^* \\cdot x$\n",
    "\n",
    "Since we cannot find the value of the population parameter $\\theta^*$ exactly, we will assume that our dataset approximates our population and use our dataset to estimate $\\theta^*$. We denote our estimation with $\\theta$, our fitted estimation with $\\hat{\\theta}$, and our model as:\n",
    "\n",
    "$$\\Large\n",
    "f_{\\theta}(x) = \\theta \\cdot x\n",
    "$$\n",
    "\n",
    "Based on this equation, define the linear model function `linear_model` below to estimate $\\textbf{y}$ (the $y$-values) given $\\textbf{x}$ (the $x$-values) and $\\theta$. This model is similar to the model you defined in Lab 5: Modeling and Estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9e0e9ad3286c43f9153aa7e6e40f9c84",
     "grade": false,
     "grade_id": "q1c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def linear_model(x, theta):\n",
    "    \"\"\"\n",
    "    Returns the estimate of y given x and theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta -- the scalar theta\n",
    "    \"\"\"\n",
    "    y = np.dot(theta, x)\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e36884f54d707ac74716f6bb333fb8b",
     "grade": true,
     "grade_id": "q1c-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert linear_model(0, 1) == 0\n",
    "assert linear_model(10, 10) == 100\n",
    "assert np.sum(linear_model(np.array([3, 5]), 3)) == 24\n",
    "assert linear_model(np.array([7, 8]), 4).mean() == 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b241c7566702f35f7c381e11545f8b66",
     "grade": false,
     "grade_id": "q1d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 1d\n",
    "In class, we learned that the $L^2$ (or squared) loss function is smooth and continuous. Let's use $L^2$ loss to evaluate our estimate $\\theta$, which we will use later to identify an optimal $\\theta$, represented as $\\hat{\\theta}$. Define the $L^2$ loss function `l2_loss` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "140a007e3b6ca72ce415efd50b245ac8",
     "grade": false,
     "grade_id": "q1d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def l2_loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Returns the average l2 loss given y and y_hat\n",
    "\n",
    "    Keyword arguments:\n",
    "    y -- the vector of true values y\n",
    "    y_hat -- the vector of predicted values y_hat\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "    if type(y) == int:\n",
    "        return (y - y_hat) ** 2\n",
    "    return np.sum(np.square(y - y_hat)) / len(y)\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4e52031bf14b7bbf16073d8c35dcd9a2",
     "grade": true,
     "grade_id": "q1d-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert l2_loss(2, 1) == 1\n",
    "assert l2_loss(2, 0) == 4 \n",
    "assert l2_loss(5, 1) == 16\n",
    "assert l2_loss(np.array([5, 6]), np.array([1, 1])) == 20.5\n",
    "assert l2_loss(np.array([1, 1, 1]), np.array([4, 1, 4])) == 6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e8f910f3775ddafaa69fdaadd91a334",
     "grade": false,
     "grade_id": "q1e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 1e\n",
    "\n",
    "First, visualize the $L^2$ loss as a function of $\\theta$, where several different values of $\\theta$ are given. Be sure to label your axes properly. You plot should look something like this:\n",
    "![avg_l2](l2_avg_loss.png)\n",
    "\n",
    "What looks like the optimal value, $\\hat{\\theta}$, based on the visualization? Set `theta_star_guess` to the value of $\\theta$ that appears to minimize our loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c6e33758b8e3f5653e808ddc0300b671",
     "grade": false,
     "grade_id": "q1e-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize(x, y, thetas):\n",
    "    \"\"\"\n",
    "    Plots the average l2 loss for given x, y as a function of theta.\n",
    "    Use the functions you wrote for linear_model and l2_loss.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    thetas -- an array containing different estimates of the scalar theta\n",
    "    \"\"\"\n",
    "    avg_loss = [l2_loss(y, linear_model(x, theta)) for theta in thetas]\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    ... # Create your plot here\n",
    "    plt.plot(thetas, avg_loss)\n",
    "    plt.xlabel(\"Theta\")\n",
    "    plt.ylabel(\"Average Loss\")\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    \n",
    "thetas = np.linspace(-1, 5, 70)\n",
    "visualize(x, y, thetas)\n",
    "\n",
    "theta_star_guess = 1.5\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ed066111e2c8396152c7fa9154367ad5",
     "grade": true,
     "grade_id": "q1e-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert l2_loss(3, 2) == 1\n",
    "assert l2_loss(0, 10) == 100\n",
    "assert 1 <= theta_star_guess <= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "43c1b29136ee761ba3e270b7f375b352",
     "grade": false,
     "grade_id": "q2a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 2: Fitting our Simple Model\n",
    "Now that we have defined a simple linear model and loss function, let's begin working on fitting our model to the data.\n",
    "\n",
    "### Question 2\n",
    "Let's confirm our visual findings for optimal $\\hat{\\theta}$.\n",
    "\n",
    "#### Question 2a\n",
    "First, find the analytical solution for the optimal $\\hat{\\theta}$ for average $L^2$ loss. Write up your solution in the cell below using LaTex.\n",
    "\n",
    "Hint: notice that we now have $\\textbf{x}$ and $\\textbf{y}$ instead of $x$ and $y$. This means that when writing the loss function $L(\\textbf{x}, \\textbf{y}, \\theta)$, you'll need to take the average of the squared losses for each $y_i$, $f_\\theta(x_i)$ pair. For tips on getting started, see chapter [chapter 10](https://www.textbook.ds100.org/ch/10/modeling_loss_functions.html) of the textbook. Note that if you click \"Open in DataHub\", you can access the LaTeX source code of the book chapter, which you might find handy for typing up your work. Show your work, i.e. don't just write the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bcca27b512c442fd1ee8f1561cf4464",
     "grade": true,
     "grade_id": "q2a-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "L(x, y, \\theta)\n",
    "&= \\frac{1}{m} \\sum_{i = 1}^{n}(\\theta \\cdot x_i - y_i)^2\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We set the derivative equal to zero and solve for $ \\theta $ to find the minimizing value of $ \\theta $:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\dfrac{\\partial L}{\\partial \\theta} \n",
    "&= \\frac{2}{n} \\sum_{i = 1}^{n}(\\theta \\cdot x_i - y_i) \\cdot x_i \\\\\n",
    "&= \\frac{2}{n}(\\theta \\cdot \\sum_{i = 1}^{n} x_i^2 - \\sum_{i = 1}^{n}x_i y_i)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "To let $\\dfrac{\\partial L}{\\partial \\theta} = 0$, we need to set $\\theta \\cdot \\sum_{i = 1}^{n} x_i^2 - \\sum_{i = 1}^{n}x_i y_i = 0$\n",
    "\n",
    "$$\n",
    "\\hat \\theta = \\dfrac{\\sum_{i = 1}^{n}x_i y_i}{\\sum_{i = 1}^{n} x_i^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "80e964018636cab351a4a0a0c5de2d75",
     "grade": false,
     "grade_id": "q2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 2b\n",
    "Now that we have the analytic solution for $\\hat{\\theta}$, implement the function `find_theta` that calculates the numerical value of $\\hat{\\theta}$ based on our data $\\textbf{x}$, $\\textbf{y}$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "94bdfd0c1ee789011fb6867d7a1f4234",
     "grade": false,
     "grade_id": "q2b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def find_theta(x, y):\n",
    "    \"\"\"\n",
    "    Find optimal theta given x and y\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    theta_opt = np.sum(x * y) / np.sum(x ** 2)\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    return theta_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1ab7ba53caf855e3a2b03122110f87ce",
     "grade": true,
     "grade_id": "q2b-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t_hat = find_theta(x, y)\n",
    "print(f'theta_opt = {t_hat}')\n",
    "\n",
    "assert 1.4 <= t_hat <= 1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8d6c0f36a3d331bb35768368e5852d8b",
     "grade": false,
     "grade_id": "q2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 2c\n",
    "Now, let's plot our loss function again using the `visualize` function. But this time, add a vertical line at the optimal value of theta (plot the line $x = \\hat{\\theta}$). Your plot should look something like this:\n",
    "![vertical_linear](vertical_linear.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "be6dfba540f44324f8e9d36b23c6d89b",
     "grade": true,
     "grade_id": "q2c-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = find_theta(x, y)\n",
    "visualize(x, y, thetas)\n",
    "plt.axvline(theta_opt, c='r')\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "07d6ec7e5bf6c21cecc00b026407373a",
     "grade": false,
     "grade_id": "q2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 2d\n",
    "We now have an optimal value for $\\theta$ that minimizes our loss. In the cell below, plot the scatter plot of the data from Question 1a (you can reuse the `scatter` function here). But this time, add the line $f_{\\hat{\\theta}}(x) = \\hat{\\theta} \\cdot \\textbf{x}$ using the $\\hat{\\theta}$ you computed above. Your plot should look something like this:\n",
    "![scatter_with_line](scatter_with_line.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de409debacae960ce0881344523b1727",
     "grade": true,
     "grade_id": "q2d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "theta_opt = find_theta(x, y)\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x, linear_model(x, theta_opt), 'r')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "32c563aa31b67ba5d79efae8d3f01dc6",
     "grade": false,
     "grade_id": "q2e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 2e\n",
    "Great! It looks like our estimator $f_{\\hat{\\theta}}(x)$ is able to capture a lot of the data with a single parameter $\\theta$. Now let's try to remove the linear portion of our model from the data to see if we missed anything. \n",
    "\n",
    "The remaining data is known as the residual, $\\textbf{r}=\\textbf{y}-\\hat{\\theta} \\cdot \\textbf{x}$. Below, write a function to find the residual and plot the residuals corresponding to $x$ in a scatter plot. Plot a horizontal line at $y=0$ to assist visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63783515265349fca282efe28c0e5253",
     "grade": true,
     "grade_id": "q2e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def visualize_residual(x, y):\n",
    "    \"\"\"\n",
    "    Plot a scatter plot of the residuals, the remaining \n",
    "    values after removing the linear model from our data.\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    \"\"\"\n",
    "    theta_opt = find_theta(x, y)\n",
    "    y_hat = linear_model(x, theta_opt)\n",
    "    r = y - y_hat\n",
    "    plt.scatter(x, r)\n",
    "    plt.axhline(0, color='r')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('residual')\n",
    "    plt.title('Residual vs. x for Linear Model')\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "visualize_residual(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "56d80bd35df5f477de8d83b6904bbeeb",
     "grade": false,
     "grade_id": "q2f",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 2f\n",
    "What does the residual look like? Do you notice a relationship between $x$ and $r$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2c1331232300d6d35575207215a7efe",
     "grade": true,
     "grade_id": "q2f-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "The residual looks like a $sin$ wave. We can infer from the plot that $x$ and $r$ have a sinusodial relationship."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e21f3bb0302da6db7993aaf6f7d19531",
     "grade": false,
     "grade_id": "part-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 3: Increasing Model Complexity\n",
    "\n",
    "It looks like the remaining data is sinusoidal, meaning our original data follows a linear function and a sinusoidal function. Let's define a new model to address this discovery and find optimal parameters to best fit the data:\n",
    "\n",
    "$$\\Large\n",
    "f_\\boldsymbol\\theta(x) = \\theta_1x + sin(\\theta_2x)\n",
    "$$\n",
    "\n",
    "Now, our model is parameterized by both $\\theta_1$ and $\\theta_2$, or composed together, $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Note that a generalized sine function $a\\sin(bx+c)$ has three parameters: amplitude scaling parameter $a$, frequency parameter $b$ and phase shifting parameter $c$. Looking at the residual plot above, it looks like the residual is zero at x = 0, and the residual swings between -1 and 1. Thus, it seems reasonable to effectively set the scaling and phase shifting parameter ($a$ and $c$ in this case) to 1 and 0 respectively. While we could try to fit $a$ and $c$, we're unlikely to get much benefit. When you're done with the homework, you can try adding $a$ and $c$ to our model and fitting these values to see if you can get a better loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e809c4a0f1ccd6c1eae60daf8562a57",
     "grade": false,
     "grade_id": "q3a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 3a\n",
    "As in Question 1, fill in the `sin_model` function that predicts $\\textbf{y}$ (the $y$-values) using $\\textbf{x}$ (the $x$-values), but this time based on our new equation.\n",
    "\n",
    "*Hint:* Try to do this without using for loops. The `np.sin` function may help you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "86bfd626c397b945138871e2810819ad",
     "grade": false,
     "grade_id": "q3a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def sin_model(x, theta_1, theta_2):\n",
    "    \"\"\"\n",
    "    Predict the estimate of y given x, theta_1, theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    theta_1 -- the scalar value theta_1\n",
    "    theta_2 -- the scalar value theta_2\n",
    "    \"\"\"\n",
    "    y = np.dot(theta_1, x) + np.sin(np.dot(theta_2, x))\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d8fe05d2e49846841ef2783b94a993f4",
     "grade": true,
     "grade_id": "q3a-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(sin_model(1, 1, np.pi), 1.0000000000000002)\n",
    "# Check that we accept x as arrays\n",
    "assert len(sin_model(x, 2, 2)) > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8b6768b6dc93c9877629947e88211283",
     "grade": false,
     "grade_id": "q3b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 3b\n",
    "Use the average $L^2$ loss to compute $\\frac{\\partial L }{\\partial \\theta_1}, \\frac{\\partial L }{\\partial \\theta_2}$. \n",
    "\n",
    "First, we will use LaTex to write $L(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2)$, $\\frac{\\partial L }{\\partial \\theta_1}$, and $\\frac{\\partial L }{\\partial \\theta_2}$ given $\\textbf{x}$, $\\textbf{y}$, $\\boldsymbol{\\theta}$.\n",
    "\n",
    "You don't need to write out the full derivation. Just the final expression is fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d4e10d1661f1b6091ac4fe1631347e1",
     "grade": true,
     "grade_id": "q3b-answer",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "$$L(\\textbf{x}, \\textbf{y}, \\theta_1, \\theta_2) = \\frac{1}{n} \\sum_{i=1}^n (\\theta_1x_i + sin(\\theta_2 x_i) - y_i) ^ 2$$\n",
    "$$\\dfrac{\\partial L }{\\partial \\theta_1} = \\frac{2}{n} \\sum_{i=1}^n (\\theta_1 x_i + sin(\\theta_2 x_i) - y_i) \\cdot x_i$$\n",
    "$$\\dfrac{\\partial L }{\\partial \\theta_2} = \\frac{2}{n} \\sum_{i=1}^n (\\theta_1 x_i + sin(\\theta_2 x_i) - y_i) \\cdot cos(\\theta_2 x_i) \\cdot x_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "35d19580a98dd4928228d773d004c9c7",
     "grade": false,
     "grade_id": "q3c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 3c\n",
    "Now, implement the functions `dt1` and `dt2`, which should compute $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ respectively. Use the formulas you wrote for $\\frac{\\partial L }{\\partial \\theta_1}$ and $\\frac{\\partial L }{\\partial \\theta_2}$ in the previous exercise. In the functions below, the parameter `theta` is a vector that looks like $( \\theta_1, \\theta_2 )$.\n",
    "\n",
    "Note: To keep your code a bit more concise, be aware that `np.mean` does the same thing as `np.sum` divided by the length of the numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "452d696d64d2f1d1f49b054e9b530e9e",
     "grade": false,
     "grade_id": "q3c-answer-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dt1(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_1\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    theta_1, theta_2 = theta[0], theta[1]\n",
    "    return np.mean(2 * (sin_model(x, theta_1, theta_2) - y) * x)\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3836f909812a44b486683321ef113601",
     "grade": false,
     "grade_id": "q3c-answer-2",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def dt2(x, y, theta):\n",
    "    \"\"\"\n",
    "    Compute the numerical value of the partial of l2 loss with respect to theta_2\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of all x values\n",
    "    y -- the vector of all y values\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    ...\n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    theta_1, theta_2 = theta[0], theta[1]\n",
    "    return np.mean(2 * (sin_model(x, theta_1, theta_2) - y) * np.cos(np.dot(theta_2, x)) * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d317c667d0df0d5772e08ae236aed1d",
     "grade": false,
     "grade_id": "dt",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This function calls dt1 and dt2 and returns the gradient dt. It is already implemented for you.\n",
    "def dt(x, y, theta):\n",
    "    \"\"\"\n",
    "    Returns the gradient of l2 loss with respect to vector theta\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    \"\"\"\n",
    "    return np.array([dt1(x,y,theta), dt2(x,y,theta)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1daa5f4404559161ad5795735d453592",
     "grade": true,
     "grade_id": "q3d-tests",
     "locked": true,
     "points": 2,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.isclose(dt1(x, y, [0, np.pi]), -25.376660670924529)\n",
    "assert np.isclose(dt2(x, y, [0, np.pi]), 1.9427210155296564)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e850eeef09c5b3eb13436d0705c8452",
     "grade": false,
     "grade_id": "q4a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 4: Gradient Descent\n",
    "Now try to solve for the optimal $\\hat{\\theta}$ analytically...\n",
    "\n",
    "**Just kidding!**\n",
    "\n",
    "You can try but we don't recommend it. When finding an analytic solution becomes difficult or impossible, we resort to alternative optimization methods for finding an approximate solution.\n",
    "\n",
    "### Question 4\n",
    "\n",
    "So let's try implementing a numerical optimization method: gradient descent!\n",
    "\n",
    "#### Question 4a\n",
    "Implement the `grad_desc` function that performs gradient descent for a finite number of iterations. This function takes in an array for $\\textbf{x}$ (`x`), an array for $\\textbf{y}$ (`y`), and an initial value for $\\theta$ (`theta`). `alpha` will be the learning rate (or step size, whichever term you prefer). In this part, we'll use a static learning rate that is the same at every time step.\n",
    "\n",
    "At each time step, use the gradient and `alpha` to update your current `theta`. Also at each time step, be sure to save the current `theta` in `theta_history`, along with the $L^2$ loss (computed with the current `theta`) in `loss_history`.\n",
    "\n",
    "Hints:\n",
    "- Write out the gradient update equation (1 step). What variables will you need for each gradient update? Of these variables, which ones do you already have, and which ones will you need to recompute at each time step?\n",
    "- You may need a loop here to update `theta` several times\n",
    "- Recall that the gradient descent update function follows the form:\n",
    "$$\\large\n",
    "\\boldsymbol\\theta^{(t+1)} \\leftarrow \\boldsymbol\\theta^{(t)} - \\alpha \\left(\\nabla_\\boldsymbol\\theta \\mathbf{L}(\\textbf{x}, \\textbf{y}, \\boldsymbol\\theta^{(t)}) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e54215c62c631397be77c4623cf53551",
     "grade": false,
     "grade_id": "init-t",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "def init_t():\n",
    "    \"\"\"Creates an initial theta [0, 0] of shape (2,) as a starting point for gradient descent\"\"\"\n",
    "    return np.zeros((2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a49963085c264dc13c9405db3f0936b3",
     "grade": false,
     "grade_id": "q4a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_desc(x, y, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and static learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta to use at first iteration\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate (also called the step size)\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    for i in range(num_iter):\n",
    "        theta = theta - alpha * dt(x, y, theta)\n",
    "        theta_history.append(theta)\n",
    "        loss = l2_loss(sin_model(x, theta[0], theta[1]), y)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    return theta, theta_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "aa3a6ebff65678ecc6ff820bb52afa98",
     "grade": true,
     "grade_id": "q4a-tests",
     "locked": true,
     "points": 3,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t, num_iter=20, alpha=0.1)\n",
    "\n",
    "assert len(ts) == len(loss) == 20 # theta history and loss history are 20 items in them\n",
    "assert ts[0].shape == (2,) # theta history contains theta values\n",
    "assert np.isscalar(loss[0]) # loss history is a list of scalar values, not vector\n",
    "\n",
    "assert loss[1] - loss[-1] > 0 # loss is decreasing\n",
    "\n",
    "assert np.allclose(np.sum(t_est), 4.5, atol=2e-1)  # theta_est should be close to our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6ef1ad6c0bcc0496c70b9564e09dcc58",
     "grade": false,
     "grade_id": "q4b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 4b\n",
    "Now, let's try using a decaying learning rate. Implement `grad_desc_decay` below, which performs gradient descent with a learning rate that decreases slightly with each time step. You should be able to copy most of your work from the previous part, but you'll need to tweak how you update `theta` at each time step.\n",
    "\n",
    "By decaying learning rate, we mean instead of just a number $\\alpha$, the learning should be now $\\frac{\\alpha}{i+1}$ where $i$ is the current number of iteration. (Why do we need to add '+ 1' in the denominator?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "74d0193bf54cb01ff12364f80ad4210b",
     "grade": false,
     "grade_id": "q4b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def grad_desc_decay(x, y, theta, num_iter=20, alpha=0.1):\n",
    "    \"\"\"\n",
    "    Run gradient descent update for a finite number of iterations and decaying learning rate\n",
    "\n",
    "    Keyword arguments:\n",
    "    x -- the vector of values x\n",
    "    y -- the vector of values y\n",
    "    theta -- the vector of values theta\n",
    "    num_iter -- the max number of iterations\n",
    "    alpha -- the learning rate\n",
    "    \n",
    "    Return:\n",
    "    theta -- the optimal value of theta after num_iter of gradient descent\n",
    "    theta_history -- the series of theta values over each iteration of gradient descent\n",
    "    loss_history -- the series of loss values over each iteration of gradient descent\n",
    "    \"\"\"\n",
    "    theta_history = []\n",
    "    loss_history = []\n",
    "    for i in range(num_iter):\n",
    "        theta = theta - (alpha / (i + 1)) * dt(x, y, theta)\n",
    "        theta_history.append(theta)\n",
    "        loss = l2_loss(sin_model(x, theta[0], theta[1]), y)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "    return theta, theta_history, loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "efbf3fa77a69781ef5a6cf1536cc174d",
     "grade": true,
     "grade_id": "q4b-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "t = init_t()\n",
    "t_est_decay, ts_decay, loss_decay = grad_desc_decay(x, y, t, num_iter=20, alpha=0.1)\n",
    "\n",
    "assert len(ts_decay) == len(loss_decay) == 20 # theta history and loss history are 20 items in them\n",
    "assert ts_decay[0].shape == (2,) # theta history contains theta values\n",
    "assert np.isscalar(loss[0]) # loss history should be a list of values, not vector\n",
    "\n",
    "assert loss_decay[1] - loss_decay[-1] > 0 # loss is decreasing\n",
    "\n",
    "assert np.allclose(np.sum(t_est_decay), 4.5, atol=2e-1)  # theta_est should be close to our value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "149c116887a45385a5a4f64f0f47da0e",
     "grade": false,
     "grade_id": "q4c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 4c\n",
    "Let's visually inspect our results of running gradient descent to optimize $\\boldsymbol\\theta$. Plot our $x$-values with our model's predicted $y$-values over the original scatter plot. Did gradient descent successfully optimize $\\boldsymbol\\theta$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e435cdf01d7746aa75f8c02102c5cf19",
     "grade": false,
     "grade_id": "q4c-answer",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "t = init_t()\n",
    "t_est, ts, loss = grad_desc(x, y, t)\n",
    "\n",
    "t = init_t()\n",
    "t_est_decay, ts_decay, loss_decay = grad_desc_decay(x, y, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c4757865c7f7455890131927bd0ee5a",
     "grade": false,
     "grade_id": "q4c-answer-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "y_pred = sin_model(x, t_est[0], t_est[1])\n",
    "\n",
    "plt.plot(x, y_pred, label='Model')\n",
    "plt.scatter(x, y, alpha=0.5, label='Observation', color='gold')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "217347beaeb3e82c29d482385b0579e0",
     "grade": true,
     "grade_id": "q4c-answer-3",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Gradient descent successfully optimized $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8a7c0340f94109b56d549928d3cb57f1",
     "grade": false,
     "grade_id": "q4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 4d\n",
    "Let's compare our two gradient descent methods and see how they differ. Plot the loss values over each iteration of gradient descent for both static learning rate and decaying learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "167b34d9b23f0e80454f7496371f07f3",
     "grade": true,
     "grade_id": "q4d-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(loss)), loss) # Plot of loss history for static learning rate\n",
    "plt.plot(np.arange(len(loss_decay)), loss_decay) # Plot of loss history for decaying learning rate\n",
    "plt.xlabel(\"Iteration #\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.title(\"Learning Rate of Constant lr and Decaying lr\")\n",
    "plt.legend([\"Constant\", \"Decaying\"])\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0214123862b643d058796e038c0108cd",
     "grade": false,
     "grade_id": "q4e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 4e\n",
    "Compare and contrast the performance of the two gradient descent methods. Which method begins to converge more quickly?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "41737c821f1d2f59c2e9e0aad86d8156",
     "grade": true,
     "grade_id": "q4e-answer",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Both methods converges but decaying step size converges more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5cec5977b834c2ad303e2e772a7c9fdc",
     "grade": false,
     "grade_id": "loss-3d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 5: Visualizing Loss\n",
    "\n",
    "### Question 5:\n",
    "Let's visualize our loss functions and gain some insight as to how gradient descent and stochastic gradient descent are optimizing our model parameters.\n",
    "\n",
    "#### Question 5a: \n",
    "In the previous plot is about the loss decrease over time, but what exactly is path the theta value? Run the following three cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "87d57683f5a0862beb13edb374dc5ef5",
     "grade": false,
     "grade_id": "loss-3d-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me\n",
    "ts = np.array(ts).squeeze()\n",
    "ts_decay = np.array(ts_decay).squeeze()\n",
    "loss = np.array(loss)\n",
    "loss_decay = np.array(loss_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2239c32003646be003bd1364d9c28266",
     "grade": false,
     "grade_id": "loss-3d-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see a 3D plot (gradient descent with static alpha)\n",
    "plot_3d(ts[:, 0], ts[:, 1], loss, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0d0ca51ea241c1c961a07fb616d3a935",
     "grade": false,
     "grade_id": "loss-3d-4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run me to see another 3D plot (gradient descent with decaying alpha)\n",
    "plot_3d(ts_decay[:, 0], ts_decay[:, 1], loss_decay, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4e13210157375396c7e56fd444ae341c",
     "grade": false,
     "grade_id": "q5a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In the following cell, write 1-2 sentences about the differences between using a static learning rate and a learning rate with decay for gradient descent. Use the loss history plot as well as the two 3D visualization to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b80de2765534fa02d53daf9e3bd683a3",
     "grade": true,
     "grade_id": "q5a-answer",
     "locked": false,
     "points": 2,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "Static learning rate vibrates a lot for having a too large step while the decaying learning rate will automatically get smaller when performing gradient descent and get to the minimum more smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8999b3efa627cccdc2587ad475e5f393",
     "grade": false,
     "grade_id": "q5b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 5b:\n",
    "\n",
    "Another common way of visualizing 3D dynamics is with a _contour_ plot. \n",
    "\n",
    "Please refer to this notebook when you are working on the next question: Please refer to this notebook when you are working on the next question: http://www.ds100.org/fa18/assets/lectures/lec09/09-Models-and-Estimation-II.html. Search the page for `go.Contour`.\n",
    "\n",
    "In next question, fill in the necessary part to create a contour plot. Then run the following cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ebc4018bfe198eda5a49a3df8363e719",
     "grade": false,
     "grade_id": "q5b-import",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Run me\n",
    "import plotly\n",
    "import plotly.graph_objs as go\n",
    "plotly.offline.init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69f6c28a6305639dd17dc0fa18b17e43",
     "grade": false,
     "grade_id": "q5b-1",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def contour_plot(title, theta_history, loss_function, model, x, y):\n",
    "    \"\"\"\n",
    "    The function takes the following as argument:\n",
    "        theta_history: a (N, 2) array of theta history\n",
    "        loss: a list or array of loss value\n",
    "        loss_function: for example, l2_loss\n",
    "        model: for example, sin_model\n",
    "        x: the original x input\n",
    "        y: the original y output\n",
    "    \"\"\"\n",
    "    theta_1_series = theta_history[:,0] # a list or array of theta_1 value\n",
    "    theta_2_series = theta_history[:,1] # a list or array of theta_2 value\n",
    "    \n",
    "    # Create trace of theta point\n",
    "#     Uncomment the following lines and fill in the TODOS\n",
    "    thata_points = go.Scatter(name=\"Theta Values\", \n",
    "                              x=theta_1_series,\n",
    "                              y=theta_2_series,\n",
    "                              mode=\"lines+markers\")\n",
    "\n",
    "    ## In the following block of code, we generate the z value\n",
    "    ## across a 2D grid\n",
    "    t1_s = np.linspace(np.min(theta_1_series) - 0.1, np.max(theta_1_series) + 0.1)\n",
    "    t2_s = np.linspace(np.min(theta_2_series) - 0.1, np.max(theta_2_series) + 0.1)\n",
    "\n",
    "    x_s, y_s = np.meshgrid(t1_s, t2_s)\n",
    "    data = np.stack([x_s.flatten(), y_s.flatten()]).T\n",
    "    ls = []\n",
    "    for t1, t2 in data:\n",
    "        l = loss_function(model(x, t1, t2), y)\n",
    "        ls.append(l)\n",
    "    z = np.array(ls).reshape(50, 50)\n",
    "    \n",
    "    # Create the contour \n",
    "    # Uncomment the following lines and fill in the TODOS\n",
    "    lr_loss_contours = go.Contour(x=t1_s,\n",
    "                                  y=t2_s,\n",
    "                                  z=z,\n",
    "                                  colorscale='Viridis', reversescale=True)\n",
    "    \n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "#     raise NotImplementedError()\n",
    "\n",
    "    plotly.offline.iplot(go.Figure(data=[lr_loss_contours, thata_points], layout={'title': title}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8ad8d7e0db145b1bd8867c927fca3b57",
     "grade": false,
     "grade_id": "q5b-2",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "contour_plot('Gradient Descent with Static Learning Rate', ts, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9347b80c2cfb388edffb528e415deed4",
     "grade": false,
     "grade_id": "q5b-3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "## Run me\n",
    "contour_plot('Gradient Descent with Decay Learning Rate', ts_decay, l2_loss, sin_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e6f0290fe23dd0dffe8df72ef9be17f8",
     "grade": false,
     "grade_id": "q5b-4",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "In the following cells, write down the answer to the following questions:\n",
    "- How do you interpret the two contour plots? \n",
    "- Compare contour plot and 3D plot, what are the pros and cons of each? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a9d3e5e005da4d99af9a65834c9006c1",
     "grade": true,
     "grade_id": "q5b-5",
     "locked": false,
     "points": 4,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "The contour plots show the same results as the 3D plots before. The contour plots use different colors to show the gradient. Just like we see before, decaying learning rate converges more quickly than constant learning rate.\n",
    "\n",
    "The contour plot can more clearly show the reduction of loss of each step. However, it's hard to tell the degree of actual change compared to 3D plot. Also, in 3D plots, it's more harder to judge the reduction of loss related to different $\\theta$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6cb06c86f463c526f43dda6d4276edc5",
     "grade": false,
     "grade_id": "improvements",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## How to Improve?\n",
    "\n",
    "### Question 5c (optional)\n",
    "Try adding the two additional model parameters for phase and amplitude that we ignored (see 3a). What are the optimal phase and amplitude values for your four parameter model? Do you get a better loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3bfbce9619412f33d852c9130ded35c1",
     "grade": true,
     "grade_id": "cell-6a5e2b7a9967a4f1",
     "locked": false,
     "points": 0,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72101e57bfa91661246de9a633b703f7",
     "grade": false,
     "grade_id": "q5d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Question 5d (optional)\n",
    "\n",
    "It looks like our basic two parameter model, a combination of a linear function and sinusoidal function, was able to almost perfectly fit our data. It turns out that many real world scenarios come from relatively simple models. \n",
    "\n",
    "At the same time, the real world can be incredibly complex and a simple model wouldn't work so well. Consider the example below; it is neither linear, nor sinusoidal, nor quadratic. \n",
    "\n",
    "Optional: Suggest how we could iteratively create a model to fit this data and how we might improve our results. \n",
    "\n",
    "Extra optional: Try and build a model that fits this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "102bfb39186aad88848d466a66a14c27",
     "grade": false,
     "grade_id": "improvements-model",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "x = []\n",
    "y = []\n",
    "for t in np.linspace(0,10*np.pi, 200):\n",
    "    r = ((t)**2)\n",
    "    x.append(r*np.cos(t))\n",
    "    y.append(r*np.sin(t))\n",
    "\n",
    "plt.scatter(x,y)  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "29221d6adbab4f24893bee38543f7ffe",
     "grade": true,
     "grade_id": "improvements-suggestion",
     "locked": false,
     "points": 0,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cec2af61ba8373f10c70758e4b744f32",
     "grade": false,
     "grade_id": "cell-0a0887ce1110253f",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "---\n",
    "## 6: Short Analytic Problems\n",
    "\n",
    "Let's work through some problems to solidify the foundations of gradient descent. If these questions are hard, consider reviewing lecture and supplementary materials. \n",
    "\n",
    "### Question 6\n",
    "\n",
    "Complete the problems below. **Show your work and solution in LaTeX**. Here are some useful examples of LaTex syntax:\n",
    "\n",
    "Summation: $\\sum_{i=1}^n a_i$\n",
    "\n",
    "Exponent: $a^2$\n",
    "\n",
    "Fraction: $\\frac{a}{b}$\n",
    "\n",
    "Multiplication: $a \\cdot b$\n",
    "\n",
    "Derivative: $\\frac{\\partial}{\\partial a}$\n",
    "\n",
    "Symbols: $\\alpha, \\beta, \\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78dfcb117901f0481894fe34475f92a9",
     "grade": false,
     "grade_id": "cell-6c028a2d3902e44f",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Convexity\n",
    "\n",
    "#### Question 6a\n",
    "\n",
    "In [lecture 8](http://www.ds100.org/fa18/syllabus#lecture-week-5), we introduced the idea of a convex function. Let $h(x) = f(x) + g(x)$ where $f,g$ are convex functions. Prove that $h$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "12526021662cd8beae8438ad14134994",
     "grade": true,
     "grade_id": "cell-bd1ad27dcf7f4d69",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "If f, g are convex functions, we will have \n",
    "\n",
    "$$tf(a)  + (1-t)f(b) \\geq f(ta + (1-t)b)$$\n",
    "\n",
    "$$tg(a)  + (1-t)g(b) \\geq g(ta + (1-t)b)$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& t h(a) + (1-t) h(b) \\\\\n",
    "= & t(f(a) + g(a)) + (1-t) (f(b) + g(b)) \\\\\n",
    "= & tf(a) + (1-t) f(b) + g(a) + (1-t)g(b) \\\\\n",
    "\\geq & f(ta + (1-t)b) + g(ta + (1-t)b) \\\\\n",
    "= & h(ta +(1-t)b) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "so $h$ is convex.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e3d302e5e6d5ed9122629a51a5918a2a",
     "grade": false,
     "grade_id": "cell-611b4eaa223f2fe3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Mutlivariable/vector calculus mechanical problems\n",
    "\n",
    "\n",
    "#### Question 6b\n",
    "Show that the sum of the squared error $$L(\\textbf{w}) = ||\\textbf{Xw}-\\textbf{y}||_2^2$$ can be expanded into $$L(\\textbf{w}) = \\textbf{w}^T \\textbf{X}^T \\textbf{X w} - 2 \\textbf{y}^T \\textbf{X w} + \\textbf{y}^T \\textbf{y}$$ using vector/matrix notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d50646553757bd5c03dc685bb3b5fbc1",
     "grade": true,
     "grade_id": "cell-d4625a836dea8730",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\textbf{w}) \n",
    "& = ||\\textbf{Xw - y}||_2^2 \\\\\n",
    "& = (\\textbf{Xw} - \\textbf{y})^T (\\textbf{Xw} - \\textbf{y}) \\\\\n",
    "& = (\\textbf{w}^T\\textbf{X}^T - \\textbf{y}^T)(\\textbf{Xw} - \\textbf{y}) \\\\\n",
    "& = (\\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{W} - \\textbf{w}^T\\textbf{X}^T\\textbf{y} - \\textbf{y}^T\\textbf{X}\\textbf{w} + \\textbf{y}^T\\textbf{y}) \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "We can find that $\\textbf{w}^T\\textbf{X}^T\\textbf{y} = (\\textbf{w}^T\\textbf{X}^T\\textbf{y})^T = \\textbf{y}^T\\textbf{X}\\textbf{w}$,\n",
    "\n",
    "Thus, $(\\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{W} - \\textbf{w}^T\\textbf{X}^T\\textbf{y} - \\textbf{y}^T\\textbf{X}\\textbf{w} + \\textbf{y}^T\\textbf{y}) = (\\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{W} - 2\\textbf{y}^T\\textbf{X}\\textbf{w} + \\textbf{y}^T\\textbf{y})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8fa6e0db0d52111bf9ec634613073a40",
     "grade": false,
     "grade_id": "cell-df8b66a28982cb2d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 6c\n",
    "Solve for the optimal $\\textbf{w}$, assuming $\\textbf{X}$ is full rank. Use the Matrix Derivative rules from [lecture 11](http://www.ds100.org/fa18/syllabus#lecture-week-6).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eada60748f104fa3923f5fd286cfa143",
     "grade": true,
     "grade_id": "cell-0c0528716c3c73ba",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "$$\n",
    "L(\\textbf{w}) = \\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{W} - 2\\textbf{y}^T\\textbf{X}\\textbf{w} + \\textbf{y}^T\\textbf{y}\n",
    "$$\n",
    "According to Matrix Derivative rules, the optimal $\\textbf{w}^*$ should satisfy: \n",
    "$$\\nabla_wL(\\textbf{w}^*) = (2 \\textbf{x}^T\\textbf{X}) \\textbf{w}^* - 2\\textbf{X}^T\\textbf{y} = 0$$\n",
    "Thus, $\\textbf{w}^* = (\\textbf{X}^T\\textbf{X})^{-1}\\textbf{X}^T\\textbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cfdb7bff60f6f7ce0bf15bd506abc4d2",
     "grade": false,
     "grade_id": "cell-c29c833cffe7e03f",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 6d\n",
    "Repeat the steps above for ridge regression as described in [lecture 12](http://www.ds100.org/fa18/syllabus#lecture-week-6). Recall that ridge regression uses the following l2 regularized sum of squared error.\n",
    "\n",
    "$$L(\\textbf{w}) = ||\\textbf{X}\\textbf{w} - \\textbf{y}||_2^2 + \\lambda ||\\textbf{w}||_2^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "18b9f81408818f5fc957b20ac703436d",
     "grade": true,
     "grade_id": "cell-471420de8eb70fbc",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "L(\\textbf{w}) \n",
    "& = ||\\textbf{X}\\textbf{w} - \\textbf{y}||_2^2 + \\lambda ||\\textbf{w}||_2^2 \\\\\n",
    "& = \\textbf{w}^T\\textbf{X}^T\\textbf{X}\\textbf{W} - 2\\textbf{y}^T\\textbf{X}\\textbf{w} + \\textbf{y}^T\\textbf{y} + \\lambda \\textbf{w}^T\\textbf{w}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "Thus, \n",
    "$$\\nabla_wL(\\textbf{w}^*) = (2 \\textbf{x}^T\\textbf{X}) \\textbf{w}^* - 2\\textbf{X}^T\\textbf{y} + 2\\lambda \\textbf{I}\\textbf{w}^* = 0$$\n",
    "Thus, $\\textbf{w}^* = (\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I})^{-1}\\textbf{X}^T\\textbf{y}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7437f5f7dea4ef0b01cb1639bbe4b01f",
     "grade": false,
     "grade_id": "cell-3c33b90429860328",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 6e\n",
    "Compare the analytic solutions of least squares and ridge regression. Why does ridge regression guarantee that we can find a unique solution? What are some of the tradeoffs (pros/cons) of using ridge regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc051a4a92a942185784cff523b3b879",
     "grade": true,
     "grade_id": "cell-432afa22feaed58a",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "First we prove the objective function is convex by calculating the Hessian matrix.\n",
    "$$\\nabla^2 L(\\textbf{w}) = 2 \\textbf{X}^T\\textbf{X} + 2\\lambda \\textbf{I}$$\n",
    "Thus, $\\forall \\textbf{w} \\neq 0, \\textbf{w}^T \\cdot (\\textbf{X}^T\\textbf{X} + \\lambda \\textbf{I}) \\textbf{w} = ||\\textbf{X} \\cdot \\textbf{w}||_2^2 + \\lambda ||\\textbf{w}||_2^2 \\geq 0 $.\n",
    "The Hessian matrix is positive so the objective function is convex. As a result, the objective function have a unique optimum point so the solution to ridge regression is unique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb776ec8007e105dcea935a4bda95d31",
     "grade": false,
     "grade_id": "cell-fea0ebf2541f33c6",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Expectation and Variance\n",
    "\n",
    "#### Question 6f\n",
    "\n",
    "In [lecture 10](http://www.ds100.org/fa18/syllabus#lecture-week-6), we completed half of the proof for the linearity of expectation. Your task in this question is to complete the second half.\n",
    "\n",
    "For reference, in lecture we showed that:\n",
    "\n",
    "$$\\mathbb{E}[aX + bY + c] = a\\mathbb{E}[X] + \\sum_{x \\in \\mathbb{X}}\\sum_{y \\in \\mathbb{Y}}P(x, y)by + c$$\n",
    "\n",
    "To complete this proof, prove that:\n",
    "\n",
    "$$b\\mathbb{E}[Y] = \\sum_{x \\in \\mathbb{X}}\\sum_{y \\in \\mathbb{Y}}P(x, y)by$$\n",
    "\n",
    "Note: You cannot simply start with the given equation and use linearity of expectation. Start with the summation on the right side and manipulate it to get the left side.\n",
    "\n",
    "Hint: What can we do with the order of the summations?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9cfa4908afa85faefac9a6f265edfa48",
     "grade": true,
     "grade_id": "cell-70eae7c3502fa266",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "b\\mathbb{E}[Y] \n",
    "& = \\sum_{x \\in \\mathbb{X}}(\\sum_{y \\in \\mathbb{Y}}P(x, y)by) \\\\\n",
    "& = \\sum_{x \\in \\mathbb{X}}P(x)by \\\\\n",
    "& = \\mathbb{E}[bY] \\\\\n",
    "& = b \\mathbb{E}[Y]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "160b8061c4b8d09e79a4db76dddeb34e",
     "grade": false,
     "grade_id": "cell-afa1058694e8cad6",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 6g\n",
    "\n",
    "Prove that if two random variables $X$ and $Y$ are independent, then $Var(X - Y) = Var(X) + Var(Y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca0cd9b9357f485ffdf6de1a5c87c657",
     "grade": true,
     "grade_id": "cell-11f6d2909a5d0afd",
     "locked": false,
     "points": 3,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "Var(X-Y) \n",
    "& = Var(X) + Var(-Y) + 2 \\cdot Cov(X, -Y) \\\\\n",
    "& = Var(X) + Var(Y) + 2 \\cdot Cov(X, Y) \n",
    "\\end{aligned}$$\n",
    "Given that $X$ and $Y$ are independent, $Cov(X, Y) = 0$, thus, $Var(X - Y) = Var(X) + Var(Y) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7fde6da4234a04bf2661cf24c4a90262",
     "grade": false,
     "grade_id": "cell-05438d5c421c7b85",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 7: Quick Regex Problems\n",
    "Here are some quick problems to review your knowledge of regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e31d0c9fd0b004326ba5afe81dd76f12",
     "grade": false,
     "grade_id": "q7a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 7a\n",
    "\n",
    "Write a regular expression to match the following strings without using the `|` operator.\n",
    "\n",
    "1. **Match:** `abcdefg`\n",
    "1. **Match:** `abcde`\n",
    "1. **Match:** `abc`\n",
    "1. **Skip:** `c abc`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "294cde6196f20e42c969f91eba9fb875",
     "grade": false,
     "grade_id": "q7a-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "regxa = r\"^abc.*\" # fill in your pattern\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ce1f90a737a764b1fe7d27f364631e1",
     "grade": true,
     "grade_id": "q7a-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert (\"|\" not in regxa)\n",
    "assert (re.search(regxa, \"abc\").group() == \"abc\") \n",
    "assert (re.search(regxa, \"abcde\").group() == \"abcde\") \n",
    "assert (re.search(regxa, \"abcdefg\").group() == \"abcdefg\")\n",
    "assert (re.search(regxa, \"c abc\") is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1bfdad2a5bc276edaaf17fafdf1147e6",
     "grade": false,
     "grade_id": "q7b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 7b\n",
    "\n",
    "Write a regular expression to match the following strings without using the `|` operator.\n",
    "\n",
    "1. **Match:** `can`\n",
    "1. **Match:** `man`\n",
    "1. **Match:** `fan`\n",
    "1. **Skip:** `dan`\n",
    "1. **Skip:** `ran`\n",
    "1. **Skip:** `pan`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4324c47a0210a66f13464dfe7281894",
     "grade": false,
     "grade_id": "q7b-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "regxb = r\"[cmf]an\" # fill in your pattern\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a774356768112786e840c5f18d3feaf",
     "grade": true,
     "grade_id": "q7b-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert (\"|\" not in regxb)\n",
    "assert (re.match(regxb, 'can').group() == \"can\") \n",
    "assert (re.match(regxb, 'fan').group() == \"fan\") \n",
    "assert (re.match(regxb, 'man').group() == \"man\") \n",
    "assert (re.match(regxb, 'dan') is None) \n",
    "assert (re.match(regxb, 'ran') is None) \n",
    "assert (re.match(regxb, 'pan') is None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44f9bb29598b6ee65c046c7a1b64f272",
     "grade": false,
     "grade_id": "q7c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 7c:\n",
    "\n",
    "Write a regular expression to extract and print the quantity and type of objects in a string. You may assume that a space separates quantity and type, ie. `\"{quantity} {type}\"`. See the example string below for more detail.\n",
    "\n",
    "1. **Hint:** use `re.findall`\n",
    "2. **Hint:** use `\\d` for digits and one of either `*` or `+`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "740303c7eb615a628fa5f0a642a2e46a",
     "grade": false,
     "grade_id": "q7c-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "text_qc = \"I've got 10 eggs that I stole from 20 gooses belonging to 30 giants.\"\n",
    "\n",
    "res_qc = re.findall(r\"(\\d+ \\w+)\", text_qc)\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "res_qc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5dbc5705606d44852358bb34bedd57ed",
     "grade": true,
     "grade_id": "q7c-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert res_qc == ['10 eggs', '20 gooses', '30 giants']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c3ddc940a3ad6d0e8b9ea1ca9d8951b0",
     "grade": false,
     "grade_id": "q7d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 7d:\n",
    "\n",
    "Write a regular expression to replace at most 2 occurrences of space, comma, or dot with a colon.\n",
    "\n",
    "**Hint:** use `re.sub(regex, \"newtext\", string, number_of_occurences)`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "186f63426a06c300f33153a1f2483406",
     "grade": false,
     "grade_id": "q7d-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "text_qd = 'Python Exercises, PHP exercises.'\n",
    "res_qd = re.sub(r\"[ ,.]\", \":\", text_qd, 2) # Hint: use re.sub()\n",
    "# YOUR CODE HERE\n",
    "# raise NotImplementedError()\n",
    "\n",
    "res_qd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "24c929f8ea53b62ea9fa2c4b23d9079c",
     "grade": true,
     "grade_id": "q7d-tests",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert res_qd == 'Python:Exercises: PHP exercises.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3fce59dc72d6db396be489d30b950b9f",
     "grade": false,
     "grade_id": "q7e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### Question 7e (optional):\n",
    "\n",
    "Write a regular expression to replace all words that are not `\"mushroom\"` with `\"badger\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5523d8c6c13d17f96d1e36c29498f008",
     "grade": false,
     "grade_id": "q7e-answer",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "text_qe = 'this is a word mushroom mushroom'\n",
    "res_qe = ... # Hint: https://www.regextester.com/94017\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "res_qe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c395a12d9031a044a05af89fc172d913",
     "grade": false,
     "grade_id": "submit",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Submission - IMPORTANT, PLEASE READ\n",
    "For this assignment and future assignments (homework and projects) you will also submit your free response and plotting questions to gradescope. To do this, you can download as PDF (`File->Download As->PDF via Latex (.pdf)`). You are responsible for submitting and tagging your answers in gradescope. For each free response and plotting question, please include:\n",
    "\n",
    "1. Relevant code used to generate the plot or inform your insights\n",
    "2. The written free response or plot\n",
    "\n",
    "We are doing this to make it easier on our graders and for you, in the case you need to submit a regrade request. Gradescope (as of now) is still better for manual grading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission\n",
    "\n",
    "You're done!\n",
    "\n",
    "Before submitting this assignment, ensure to:\n",
    "\n",
    "1. Restart the Kernel (in the menubar, select Kernel->Restart & Run All)\n",
    "2. Validate the notebook by clicking the \"Validate\" button\n",
    "\n",
    "Finally, make sure to **submit** the assignment via the Assignments tab in Datahub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
